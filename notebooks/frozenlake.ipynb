{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving OpenAI's [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/)\n",
    "\n",
    "**We control an agent on a ice field and need to figure out how to best get it past holes to the goal.**\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "We can take the following actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moves(left=0, down=1, right=2, up=3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Moves = namedtuple(\"Moves\", \"left down right up\".split())\n",
    "moves = Moves(0,1,2,3)\n",
    "moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(4), Discrete(16), (0, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.action_space, env.observation_space, env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our agent can take 4 actions (left, down, right, up) and has 16 places to go to (or the state).\n",
    "\n",
    "Moving the agent randomly to see whats happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "12 0.0 True {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(f\"We are starting at state {state}\")\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    observation, reward, done, info = env.step(random.choice(moves))\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print(observation, reward, done, info)\n",
    "    sleep(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent is dumb - falls into a hole all the time! So moving on to q tables.\n",
    "\n",
    "For each observation or state we have 4 possible actions, and I'm going to store the Q value for each state/action pair in a 2d array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a state s:\n",
    "\n",
    "`np.max(Q[s])` gives us the max Q value of that state\n",
    "`np.argmax(Q[s])` gives us the index of the max step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {((5, 6), 1): 0, ((5, 3), 1): 90, ((5, 3), 2): 85})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, val in Q.Q.items():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
